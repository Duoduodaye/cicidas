#!/usr/bin/env python3
"""
AI News Fetcher - Production Version for Cicadas
è‡ªåŠ¨è·å–å’Œæ›´æ–°AIæ–°é—»æ•°æ®

ä½¿ç”¨æ–¹æ³•ï¼š
python ai_news_fetcher.py

è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
NEWS_API_KEY=your_api_key_here
"""

import os
import sys
import json
import requests
import feedparser
from datetime import datetime, timedelta
from dotenv import load_dotenv
import re

class CicadasAINews:
    def __init__(self):
        # åŠ è½½ç¯å¢ƒå˜é‡
        load_dotenv()
        
        self.news_api_key = os.getenv('NEWS_API_KEY')
        self.max_articles = int(os.getenv('MAX_ARTICLES', 15))
        self.debug = os.getenv('DEBUG_MODE', 'false').lower() == 'true'
        
        # RSS feeds from major AI companies
        self.rss_sources = [
            {
                'url': 'https://openai.com/blog/rss/',
                'name': 'OpenAI Blog',
                'category': 'research'
            },
            {
                'url': 'https://blog.anthropic.com/rss',
                'name': 'Anthropic',
                'category': 'research'
            },
            {
                'url': 'https://ai.meta.com/blog/feed/',
                'name': 'Meta AI',
                'category': 'research'
            }
        ]
        
        # AI keywords for NewsAPI
        self.ai_keywords = [
            'artificial intelligence OR AI',
            'machine learning',
            'GPT OR ChatGPT',
            'OpenAI',
            'Google AI OR DeepMind',
            'large language model OR LLM'
        ]
        
        self.log(f"ğŸ¤– AI News Fetcher initialized")
        self.log(f"ğŸ“Š Max articles: {self.max_articles}")
    
    def log(self, message):
        """æ—¥å¿—è¾“å‡º"""
        timestamp = datetime.now().strftime('%H:%M:%S')
        print(f"[{timestamp}] {message}")
        
        if self.debug:
            with open('ai_news_debug.log', 'a', encoding='utf-8') as f:
                f.write(f"{datetime.now().isoformat()} - {message}\n")
    
    def fetch_rss_news(self):
        """ä»RSS feedsè·å–æ–°é—»"""
        self.log("ğŸ“¡ Fetching RSS feeds...")
        all_news = []
        
        for source in self.rss_sources:
            try:
                self.log(f"  ğŸ“° Fetching: {source['name']}")
                feed = feedparser.parse(source['url'])
                
                for entry in feed.entries[:5]:  # æ¯ä¸ªæºå–5æ¡æœ€æ–°
                    # è¿‡æ»¤æœ€è¿‘7å¤©çš„æ–°é—»
                    pub_date = self.parse_date(entry.get('published', ''))
                    if pub_date and (datetime.now() - datetime.fromisoformat(pub_date)).days > 7:
                        continue
                    
                    news_item = {
                        'id': self.generate_id(entry.title),
                        'title': self.clean_text(entry.title),
                        'summary': self.clean_text(entry.get('summary', '')[:300]),
                        'source': source['name'],
                        'author': self.extract_author(entry),
                        'url': entry.link,
                        'publishDate': pub_date or datetime.now().strftime('%Y-%m-%d'),
                        'category': source['category'],
                        'tags': self.extract_tags(entry.title + ' ' + entry.get('summary', '')),
                        'readTime': self.estimate_read_time(entry.get('summary', '')),
                        'featured': self.is_featured_news(entry.title),
                        'language': 'en'
                    }
                    all_news.append(news_item)
                    
                self.log(f"  âœ… Got {len([n for n in all_news if n['source'] == source['name']])} articles from {source['name']}")
                
            except Exception as e:
                self.log(f"  âŒ Error fetching {source['name']}: {e}")
        
        return all_news
    
    def fetch_news_api(self):
        """ä½¿ç”¨NewsAPIè·å–æ–°é—»"""
        if not self.news_api_key:
            self.log("âš ï¸  NewsAPI key not found, skipping NewsAPI")
            return []
        
        self.log("ğŸ” Fetching from NewsAPI...")
        all_news = []
        
        for keyword in self.ai_keywords:
            try:
                self.log(f"  ğŸ” Searching: {keyword}")
                
                url = "https://newsapi.org/v2/everything"
                params = {
                    'q': keyword,
                    'language': 'en',
                    'sortBy': 'publishedAt',
                    'pageSize': 10,
                    'apiKey': self.news_api_key,
                    'from': (datetime.now() - timedelta(days=3)).strftime('%Y-%m-%d'),
                    'domains': 'techcrunch.com,arstechnica.com,theverge.com,venturebeat.com,thenextweb.com'
                }
                
                response = requests.get(url, params=params, timeout=10)
                response.raise_for_status()
                data = response.json()
                
                if 'articles' in data:
                    for article in data['articles']:
                        if not article['title'] or 'removed' in article['title'].lower():
                            continue
                            
                        news_item = {
                            'id': self.generate_id(article['title']),
                            'title': self.clean_text(article['title']),
                            'summary': self.clean_text(article['description'] or ''),
                            'source': article['source']['name'],
                            'author': article.get('author', 'AI News Team'),
                            'url': article['url'],
                            'publishDate': article['publishedAt'][:10],
                            'category': self.categorize_news(article['title']),
                            'tags': self.extract_tags(article['title'] + ' ' + (article['description'] or '')),
                            'readTime': self.estimate_read_time(article['description'] or ''),
                            'featured': self.is_featured_news(article['title']),
                            'language': 'en'
                        }
                        all_news.append(news_item)
                
                self.log(f"  âœ… Got {len([n for n in all_news if keyword.split()[0].lower() in n['title'].lower()])} articles for '{keyword}'")
                
            except Exception as e:
                self.log(f"  âŒ Error with keyword '{keyword}': {e}")
        
        return all_news
    
    def translate_to_spanish(self, news_list):
        """å°†æ–°é—»ç¿»è¯‘æˆè¥¿ç­ç‰™è¯­ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰"""
        self.log("ğŸŒ Translating to Spanish...")
        
        # ç®€å•çš„å…³é”®è¯ç¿»è¯‘æ˜ å°„
        translations = {
            'artificial intelligence': 'inteligencia artificial',
            'machine learning': 'aprendizaje automÃ¡tico',
            'deep learning': 'aprendizaje profundo',
            'neural network': 'red neuronal',
            'chatbot': 'chatbot',
            'breakthrough': 'avance',
            'research': 'investigaciÃ³n',
            'announces': 'anuncia',
            'launches': 'lanza',
            'develops': 'desarrolla',
            'AI': 'IA'
        }
        
        for news in news_list:
            if news['language'] == 'en':
                # ç®€å•ç¿»è¯‘æ ‡é¢˜ä¸­çš„å…³é”®è¯
                spanish_title = news['title']
                for en, es in translations.items():
                    spanish_title = re.sub(f'\\b{en}\\b', es, spanish_title, flags=re.IGNORECASE)
                
                # å¦‚æœæ ‡é¢˜æœ‰æ˜æ˜¾å˜åŒ–ï¼Œåˆ›å»ºè¥¿ç­ç‰™è¯­ç‰ˆæœ¬
                if spanish_title != news['title']:
                    news['title_es'] = spanish_title
                    news['language'] = 'multilingual'
        
        return news_list
    
    def clean_text(self, text):
        """æ¸…ç†æ–‡æœ¬"""
        if not text:
            return ''
        
        # ç§»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        # æ ‡å‡†åŒ–ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text).strip()
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[^\w\s\-.,!?;:()\[\]\'\"Ã¡Ã©Ã­Ã³ÃºÃ±]', '', text)
        
        return text
    
    def categorize_news(self, title):
        """æ–°é—»åˆ†ç±»"""
        title_lower = title.lower()
        
        if any(word in title_lower for word in ['research', 'study', 'paper', 'breakthrough', 'discovery']):
            return 'research'
        elif any(word in title_lower for word in ['company', 'business', 'enterprise', 'market', 'stock']):
            return 'industry'
        elif any(word in title_lower for word in ['tool', 'app', 'software', 'platform', 'api']):
            return 'tools'
        elif any(word in title_lower for word in ['ethics', 'bias', 'safety', 'regulation', 'policy']):
            return 'ethics'
        elif any(word in title_lower for word in ['startup', 'funding', 'investment', 'vc']):
            return 'startups'
        else:
            return 'research'
    
    def extract_tags(self, text):
        """æå–æ ‡ç­¾"""
        tags = []
        text_upper = text.upper()
        
        # é¢„å®šä¹‰æ ‡ç­¾
        tag_patterns = {
            'GPT': ['GPT', 'CHATGPT'],
            'OpenAI': ['OPENAI'],
            'Google': ['GOOGLE', 'DEEPMIND', 'BARD'],
            'Microsoft': ['MICROSOFT', 'COPILOT'],
            'Meta': ['META', 'FACEBOOK'],
            'Anthropic': ['ANTHROPIC', 'CLAUDE'],
            'Machine Learning': ['MACHINE LEARNING', 'ML'],
            'Deep Learning': ['DEEP LEARNING', 'NEURAL NETWORK'],
            'Computer Vision': ['COMPUTER VISION', 'IMAGE RECOGNITION'],
            'NLP': ['NATURAL LANGUAGE', 'NLP', 'TEXT'],
            'Robotics': ['ROBOT', 'ROBOTICS'],
            'AI Safety': ['AI SAFETY', 'BIAS', 'ETHICS'],
            'LLM': ['LARGE LANGUAGE MODEL', 'LLM']
        }
        
        for tag, patterns in tag_patterns.items():
            if any(pattern in text_upper for pattern in patterns):
                tags.append(tag)
        
        return tags[:5]  # æœ€å¤š5ä¸ªæ ‡ç­¾
    
    def extract_author(self, entry):
        """æå–ä½œè€…ä¿¡æ¯"""
        author = entry.get('author', '')
        if not author and 'authors' in entry:
            authors = entry.get('authors', [])
            if authors:
                author = authors[0].get('name', '')
        
        return author or 'AI News Team'
    
    def estimate_read_time(self, text):
        """ä¼°ç®—é˜…è¯»æ—¶é—´"""
        if not text:
            return '2 min'
        
        words = len(text.split())
        minutes = max(1, round(words / 200))  # å‡è®¾æ¯åˆ†é’Ÿ200è¯
        return f'{minutes} min'
    
    def is_featured_news(self, title):
        """åˆ¤æ–­æ˜¯å¦ä¸ºé‡ç‚¹æ–°é—»"""
        featured_keywords = [
            'breakthrough', 'launches', 'announces', 'releases', 'unveils',
            'gpt-5', 'gpt-4', 'agi', 'artificial general intelligence',
            'billion', 'million', 'funding', 'acquisition'
        ]
        
        title_lower = title.lower()
        return any(keyword in title_lower for keyword in featured_keywords)
    
    def parse_date(self, date_str):
        """è§£ææ—¥æœŸ"""
        if not date_str:
            return None
            
        try:
            # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
            formats = [
                '%a, %d %b %Y %H:%M:%S %Z',
                '%Y-%m-%dT%H:%M:%S%z',
                '%Y-%m-%d %H:%M:%S',
                '%Y-%m-%d'
            ]
            
            for fmt in formats:
                try:
                    dt = datetime.strptime(date_str.replace('GMT', '+0000'), fmt)
                    return dt.strftime('%Y-%m-%d')
                except:
                    continue
                    
        except Exception:
            pass
        
        return None
    
    def generate_id(self, title):
        """ç”Ÿæˆæ–°é—»ID"""
        import hashlib
        return hashlib.md5(title.encode()).hexdigest()[:8]
    
    def deduplicate_news(self, news_list):
        """å»é‡æ–°é—»"""
        seen_titles = set()
        unique_news = []
        
        for news in news_list:
            title_key = news['title'].lower().strip()
            if title_key not in seen_titles:
                seen_titles.add(title_key)
                unique_news.append(news)
        
        return unique_news
    
    def get_latest_news(self):
        """è·å–æœ€æ–°æ–°é—»"""
        self.log("ğŸš€ Starting AI news fetch...")
        
        # è·å–æ–°é—»
        rss_news = self.fetch_rss_news()
        api_news = self.fetch_news_api()
        
        # åˆå¹¶æ–°é—»
        all_news = rss_news + api_news
        self.log(f"ğŸ“Š Total articles fetched: {len(all_news)}")
        
        # å»é‡
        unique_news = self.deduplicate_news(all_news)
        self.log(f"ğŸ“Š Unique articles: {len(unique_news)}")
        
        # ç¿»è¯‘
        unique_news = self.translate_to_spanish(unique_news)
        
        # æŒ‰æ—¥æœŸå’Œé‡è¦æ€§æ’åº
        unique_news.sort(key=lambda x: (x['featured'], x['publishDate']), reverse=True)
        
        # é™åˆ¶æ•°é‡
        latest_news = unique_news[:self.max_articles]
        
        self.log(f"âœ… Final articles selected: {len(latest_news)}")
        self.log(f"â­ Featured articles: {len([n for n in latest_news if n['featured']])}")
        
        return latest_news
    
    def save_for_website(self, news_data):
        """ä¿å­˜ä¸ºç½‘ç«™æ ¼å¼"""
        self.log("ğŸ’¾ Saving news data for website...")
        
        # æ·»åŠ å…ƒæ•°æ®
        output_data = {
            'lastUpdated': datetime.now().isoformat(),
            'totalArticles': len(news_data),
            'featuredCount': len([n for n in news_data if n['featured']]),
            'categories': list(set(n['category'] for n in news_data)),
            'sources': list(set(n['source'] for n in news_data)),
            'news': news_data
        }
        
        # ä¿å­˜JSONæ•°æ®
        json_path = 'ai_news_data.json'
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        self.log(f"ğŸ“„ JSON data saved to: {json_path}")
        
        # ç”ŸæˆJavaScriptæ ¼å¼
        js_content = f"""// AI News Data - Auto-generated on {datetime.now().strftime('%Y-%m-%d %H:%M')}
// Last updated: {output_data['lastUpdated']}
// Total articles: {output_data['totalArticles']}

const aiNewsData = {json.dumps(output_data, indent=2, ensure_ascii=False)};

// For backward compatibility
const mockNews = aiNewsData.news;

// Export for modules
if (typeof module !== 'undefined' && module.exports) {{
    module.exports = aiNewsData;
}}
"""
        
        js_path = 'ai_news_data.js'
        with open(js_path, 'w', encoding='utf-8') as f:
            f.write(js_content)
        
        self.log(f"ğŸ“„ JavaScript data saved to: {js_path}")
        
        return output_data

def main():
    """ä¸»å‡½æ•°"""
    try:
        fetcher = CicadasAINews()
        
        # æ£€æŸ¥APIå¯†é’¥
        if not fetcher.news_api_key:
            print("âŒ ERROR: NEWS_API_KEY not found!")
            print("Please set your NewsAPI key in the .env file")
            print("Create .env file with: NEWS_API_KEY=your_api_key_here")
            sys.exit(1)
        
        # è·å–æ–°é—»
        latest_news = fetcher.get_latest_news()
        
        # ä¿å­˜æ•°æ®
        saved_data = fetcher.save_for_website(latest_news)
        
        # è¾“å‡ºæ‘˜è¦
        print("\n" + "="*60)
        print(f"ğŸ‰ AI News Update Complete!")
        print(f"ğŸ“… Updated: {saved_data['lastUpdated']}")
        print(f"ğŸ“Š Articles: {saved_data['totalArticles']}")
        print(f"â­ Featured: {saved_data['featuredCount']}")
        print(f"ğŸ“‚ Categories: {', '.join(saved_data['categories'])}")
        print(f"ğŸ“° Sources: {', '.join(saved_data['sources'])}")
        print("="*60)
        
        # æ˜¾ç¤ºå‰å‡ æ¡æ–°é—»æ ‡é¢˜
        print("\nğŸ“‹ Latest Headlines:")
        for i, news in enumerate(latest_news[:5], 1):
            status = "â­" if news['featured'] else "ğŸ“°"
            print(f"{i}. {status} {news['title']} ({news['source']})")
        
        print(f"\nğŸ’¾ Files saved:")
        print(f"  - ai_news_data.json")
        print(f"  - ai_news_data.js")
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        if fetcher.debug:
            import traceback
            traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()