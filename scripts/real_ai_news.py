#!/usr/bin/env python3
"""
Real AI News Fetcher - Production Version
è·å–çœŸå®çš„AIæ–°é—»æ•°æ®

ä½¿ç”¨æ–¹æ³•ï¼š
1. æ³¨å†ŒAPIå¯†é’¥
2. é…ç½®ç¯å¢ƒå˜é‡
3. è¿è¡Œè„šæœ¬è·å–æœ€æ–°æ–°é—»
"""

import requests
import json
import os
from datetime import datetime, timedelta
import feedparser  # RSS feeds
import re

class AINewsAggregator:
    def __init__(self):
        self.news_api_key = os.environ.get('NEWS_API_KEY')  # newsapi.org
        self.sources = {
            'rss_feeds': [
                'https://openai.com/blog/rss/',
                'https://deepmind.google/blog/rss.xml',
                'https://blog.anthropic.com/rss/',
                'https://www.microsoft.com/en-us/research/feed/',
                'https://ai.meta.com/blog/feed/',
            ],
            'news_keywords': [
                'artificial intelligence',
                'machine learning', 
                'GPT',
                'ChatGPT',
                'OpenAI',
                'Google AI',
                'DeepMind'
            ]
        }
    
    def fetch_rss_news(self):
        """ä»RSS feedsè·å–æ–°é—»"""
        all_news = []
        
        for feed_url in self.sources['rss_feeds']:
            try:
                feed = feedparser.parse(feed_url)
                for entry in feed.entries[:5]:  # å–æ¯ä¸ªæºæœ€æ–°5æ¡
                    news_item = {
                        'title': entry.title,
                        'summary': self.clean_text(entry.summary if hasattr(entry, 'summary') else ''),
                        'source': feed.feed.title if hasattr(feed.feed, 'title') else 'AI Blog',
                        'url': entry.link,
                        'date': self.parse_date(entry.published if hasattr(entry, 'published') else ''),
                        'category': self.categorize_news(entry.title),
                        'tags': self.extract_tags(entry.title + ' ' + (entry.summary if hasattr(entry, 'summary') else '')),
                        'featured': 'GPT' in entry.title or 'breakthrough' in entry.title.lower()
                    }
                    all_news.append(news_item)
                    
            except Exception as e:
                print(f"Error fetching RSS from {feed_url}: {e}")
                
        return all_news
    
    def fetch_news_api(self):
        """ä½¿ç”¨NewsAPIè·å–æ–°é—»"""
        if not self.news_api_key:
            return []
            
        all_news = []
        
        for keyword in self.sources['news_keywords']:
            try:
                url = f"https://newsapi.org/v2/everything"
                params = {
                    'q': keyword,
                    'language': 'en',
                    'sortBy': 'publishedAt',
                    'pageSize': 10,
                    'apiKey': self.news_api_key,
                    'from': (datetime.now() - timedelta(days=3)).strftime('%Y-%m-%d')
                }
                
                response = requests.get(url, params=params)
                data = response.json()
                
                if 'articles' in data:
                    for article in data['articles']:
                        news_item = {
                            'title': article['title'],
                            'summary': self.clean_text(article['description'] or ''),
                            'source': article['source']['name'],
                            'url': article['url'],
                            'date': article['publishedAt'][:10],
                            'category': self.categorize_news(article['title']),
                            'tags': self.extract_tags(article['title'] + ' ' + (article['description'] or '')),
                            'featured': any(term in article['title'].lower() for term in ['breakthrough', 'launches', 'announces'])
                        }
                        all_news.append(news_item)
                        
            except Exception as e:
                print(f"Error fetching news for '{keyword}': {e}")
                
        return all_news
    
    def clean_text(self, text):
        """æ¸…ç†æ–‡æœ¬å†…å®¹"""
        text = re.sub(r'<[^>]+>', '', text)  # ç§»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'\s+', ' ', text).strip()  # æ ‡å‡†åŒ–ç©ºæ ¼
        return text[:200] + '...' if len(text) > 200 else text
    
    def categorize_news(self, title):
        """æ ¹æ®æ ‡é¢˜åˆ†ç±»æ–°é—»"""
        title_lower = title.lower()
        
        if any(word in title_lower for word in ['research', 'study', 'paper', 'breakthrough']):
            return 'research'
        elif any(word in title_lower for word in ['company', 'business', 'enterprise', 'market']):
            return 'industry'
        elif any(word in title_lower for word in ['tool', 'app', 'software', 'platform']):
            return 'tools'
        elif any(word in title_lower for word in ['ethics', 'bias', 'safety', 'regulation']):
            return 'ethics'
        elif any(word in title_lower for word in ['startup', 'funding', 'investment']):
            return 'startups'
        else:
            return 'research'
    
    def extract_tags(self, text):
        """æå–å…³é”®è¯æ ‡ç­¾"""
        common_tags = [
            'GPT', 'ChatGPT', 'OpenAI', 'Google', 'DeepMind', 'Anthropic',
            'Microsoft', 'Meta', 'AI Safety', 'Machine Learning', 'LLM',
            'Computer Vision', 'NLP', 'Robotics', 'AGI'
        ]
        
        found_tags = []
        text_upper = text.upper()
        
        for tag in common_tags:
            if tag.upper() in text_upper:
                found_tags.append(tag)
                
        return found_tags[:5]  # æœ€å¤š5ä¸ªæ ‡ç­¾
    
    def parse_date(self, date_str):
        """è§£ææ—¥æœŸ"""
        try:
            if date_str:
                return datetime.fromisoformat(date_str.replace('Z', '+00:00')).strftime('%Y-%m-%d')
        except:
            pass
        return datetime.now().strftime('%Y-%m-%d')
    
    def deduplicate_news(self, news_list):
        """å»é‡æ–°é—»"""
        seen_titles = set()
        unique_news = []
        
        for news in news_list:
            if news['title'] not in seen_titles:
                seen_titles.add(news['title'])
                unique_news.append(news)
                
        return unique_news
    
    def get_latest_news(self):
        """è·å–æœ€æ–°æ–°é—»"""
        print("ğŸ¤– å¼€å§‹è·å–æœ€æ–°AIæ–°é—»...")
        
        # ä»å¤šä¸ªæ¥æºè·å–æ–°é—»
        rss_news = self.fetch_rss_news()
        api_news = self.fetch_news_api()
        
        # åˆå¹¶å¹¶å»é‡
        all_news = rss_news + api_news
        unique_news = self.deduplicate_news(all_news)
        
        # æŒ‰æ—¥æœŸæ’åº
        unique_news.sort(key=lambda x: x['date'], reverse=True)
        
        # é€‰æ‹©æœ€æ–°15æ¡
        latest_news = unique_news[:15]
        
        print(f"ğŸ“° è·å–åˆ° {len(latest_news)} æ¡æœ€æ–°AIæ–°é—»")
        
        return latest_news
    
    def save_to_website(self, news_data):
        """ä¿å­˜åˆ°ç½‘ç«™æ ¼å¼"""
        # ç”ŸæˆJavaScriptæ ¼å¼çš„æ•°æ®
        js_data = f"""
// è‡ªåŠ¨ç”Ÿæˆçš„AIæ–°é—»æ•°æ® - {datetime.now().strftime('%Y-%m-%d %H:%M')}
const mockNews = {json.dumps(news_data, indent=2, ensure_ascii=False)};
        """
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open('ai_news_data.js', 'w', encoding='utf-8') as f:
            f.write(js_data)
            
        print("ğŸ’¾ æ–°é—»æ•°æ®å·²ä¿å­˜åˆ° ai_news_data.js")
        
        # ç”ŸæˆHTMLç‰‡æ®µ
        html_content = self.generate_html_snippet(news_data)
        with open('ai_news_snippet.html', 'w', encoding='utf-8') as f:
            f.write(html_content)
            
        print("ğŸŒ HTMLç‰‡æ®µå·²ä¿å­˜åˆ° ai_news_snippet.html")
    
    def generate_html_snippet(self, news_data):
        """ç”ŸæˆHTMLç‰‡æ®µ"""
        featured_news = [n for n in news_data if n.get('featured', False)]
        regular_news = [n for n in news_data if not n.get('featured', False)]
        
        html = f"""
<div class="ai-news-update">
    <h3>ğŸ¤– æœ€æ–°AIæ–°é—» - {datetime.now().strftime('%Y-%m-%d')}</h3>
    
    <div class="featured-news">
        <h4>â­ é‡ç‚¹æ–°é—»</h4>
"""
        
        for news in featured_news[:3]:
            html += f"""
        <div class="news-item featured">
            <h5><a href="{news['url']}" target="_blank">{news['title']}</a></h5>
            <p>{news['summary']}</p>
            <div class="meta">{news['source']} â€¢ {news['date']}</div>
            <div class="tags">{''.join([f'<span class="tag">#{tag}</span>' for tag in news['tags']])}</div>
        </div>
"""
        
        html += """
    </div>
    <div class="other-news">
        <h4>ğŸ“° å…¶ä»–æ–°é—»</h4>
"""
        
        for news in regular_news[:8]:
            html += f"""
        <div class="news-item">
            <h6><a href="{news['url']}" target="_blank">{news['title']}</a></h6>
            <p>{news['summary']}</p>
            <div class="meta">{news['source']} â€¢ {news['date']}</div>
        </div>
"""
        
        html += """
    </div>
</div>
        """
        
        return html

def main():
    """ä¸»å‡½æ•°"""
    aggregator = AINewsAggregator()
    latest_news = aggregator.get_latest_news()
    aggregator.save_to_website(latest_news)
    
    print("âœ… AIæ–°é—»æ›´æ–°å®Œæˆï¼")
    print("ğŸ“‹ ä½¿ç”¨è¯´æ˜ï¼š")
    print("1. å°† ai_news_data.js å†…å®¹å¤åˆ¶åˆ°ä½ çš„ç½‘é¡µä¸­")
    print("2. æˆ–ç›´æ¥ä½¿ç”¨ ai_news_snippet.html ç‰‡æ®µ")
    print("3. è®¾ç½®å®šæ—¶ä»»åŠ¡æ¯æ—¥è‡ªåŠ¨è¿è¡Œæ­¤è„šæœ¬")

if __name__ == "__main__":
    main()